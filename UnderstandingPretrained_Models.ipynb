{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNa8PfzgdpYT7gvyJ5AJvJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sra1panasa/NLP/blob/main/UnderstandingPretrained_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequance/sentance  classification (sentance is +ve or -ve):** \n",
        "* In transformers library we have a class/model already available with with we can make classifications.\n",
        "* Don't get confuse with sequance it is like sentance only.\n",
        "* simple example helps to understand transormer library pretrained model. \n",
        "https://www.analyticsvidhya.com/blog/2021/09/a-deep-dive-into-transformers-library/\n",
        "\n",
        "* Before going into deep details we must understand tokenizer.\n",
        "\n",
        "  (https://huggingface.co/docs/transformers/preprocessing)\n",
        "\n",
        "  (https://www.philschmid.de/huggingface-transformers-keras-tf)\n",
        "\n",
        "  https://www.fatalerrors.org/a/transformers-parsing-sequence-annotation-task-datawhale-reading-notes-7.html\n"
      ],
      "metadata": {
        "id": "5jl40ooztKaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA6H7ObHto4F",
        "outputId": "4dae7c88-8ea6-4a5e-dcce-9b8f325fa2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "hf_pretrained_tokenizer_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_pretrained_tokenizer_checkpoint)\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
        "    \"I am very excited about training the model !!\",\n",
        "    \"I hate this weather which makes me feel irritated  !\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yntPzOBcXoPT",
        "outputId": "e0419c9a-0d9d-4f14-f3eb-0d820eb5fcd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  2572,  2200,  7568,  2055,  2731,  1996,  2944,   999,\n",
            "           999,   102,     0,     0,     0,     0],\n",
            "        [  101,  1045,  5223,  2023,  4633,  2029,  3084,  2033,  2514, 15560,\n",
            "           999,   102,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "# Downloading the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "#AutoModelForSequenceClassification this model helps for sequeance classification.. whether the seq is +ve or -ve..\n",
        "#here wt is sequeance .. sequance means sequance of words..\n"
      ],
      "metadata": {
        "id": "tXwftC5OqLzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Seq1 example - it has only one sequance..\n",
        "\n",
        "  raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\"]\n",
        "* seq3 example:\n",
        "\n",
        "    raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
        "    \"I am very excited about training the model !!\",\n",
        "    \"I hate this weather which makes me feel irritated  !\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "lzlzjgFVsnfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We now have the tokenized inputs ready. Now letâ€™s get the predictions,"
      ],
      "metadata": {
        "id": "HH7A2zSzzvi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2cxeeiVq5To",
        "outputId": "bacf2945-3e51-4aba-d343-e43658540a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFuZ5R17rKSy",
        "outputId": "1abdc620-75dc-45a3-99d6-c92ecfe8d2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can see that our model has returned a 1 x 2 matrix, (since we have 1 sequences in input and there are 2 classes)."
      ],
      "metadata": {
        "id": "qeFaveFl0Blz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "outputs = torch.nn.functional.softmax(outputs.logits, dim = -1)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJvpvbgYrRdZ",
        "outputId": "5839a350-f25e-4003-f6f7-613373532308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0402, 0.9598]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We applied the softmax activation to our outputs for easy understanding,\n",
        "* We can see that the outputs are passed through softmax activation to get the probabilities of each class for the input sentences.\n",
        "\n",
        "* We have [0.04, 0.95] as the output for the first input, [0.0002, 0.99] as the output for the second input, and finally [0.99, 0.000006] as the output for the third input sample."
      ],
      "metadata": {
        "id": "P3WueOcy0ZxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If we check the labels of the model"
      ],
      "metadata": {
        "id": "V1xkMJZL0vgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tD1vQnirpVz",
        "outputId": "0f33410a-9d09-4df9-bcea-ccf5c0665d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "* We have seen that we can download the pre-trained models and tokenizers and use them directly on our own dataset. From the above example, we have seen that the pre-trained model was able to classify the label/sentiment of input sequences with almost 100% confidence.\n",
        "\n",
        "* Hence we can conclude that these pre-trained models can be used to creating state-of-the-art NLP systems and by fine-tuning it on our own datasets we can be able to get awesome results by yielding higher scores in the required metric."
      ],
      "metadata": {
        "id": "Uo2YTQMyNyTe"
      }
    }
  ]
}